{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMkPIVk7HD/19FfzumwqzNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnovaYoung/AI-System-for-Image-Restoration-and-Enhancement/blob/Modeling/Real_ESRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYalmOwm6W58",
        "outputId": "de99e638-d54b-4b44-91f2-ec3b6a8eec27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracting dataset...\n",
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/My Drive/super_resolution_resized_split.zip\"  # Adjust this path if necessary\n",
        "extract_path = \"/content/super_resolution_dataset\"\n",
        "\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNSAMPLE DATASET\n",
        "\n"
      ],
      "metadata": {
        "id": "JuQgQlYb64vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Original directories\n",
        "original_dirs = {\n",
        "    \"train_lr\": \"/content/super_resolution_dataset/train/super_resolution_lr\",\n",
        "    \"train_hr\": \"/content/super_resolution_dataset/train/super_resolution_hr\",\n",
        "    \"val_lr\": \"/content/super_resolution_dataset/val/super_resolution_lr\",\n",
        "    \"val_hr\": \"/content/super_resolution_dataset/val/super_resolution_hr\",\n",
        "    \"test_lr\": \"/content/super_resolution_dataset/test/super_resolution_lr\",\n",
        "    \"test_hr\": \"/content/super_resolution_dataset/test/super_resolution_hr\",\n",
        "}\n",
        "\n",
        "# Reduced dataset directories\n",
        "reduced_base_dir = \"/content/super_resolution_dataset/reduced\"\n",
        "reduced_dirs = {\n",
        "    \"train_lr\": os.path.join(reduced_base_dir, \"train/super_resolution_lr\"),\n",
        "    \"train_hr\": os.path.join(reduced_base_dir, \"train/super_resolution_hr\"),\n",
        "    \"val_lr\": os.path.join(reduced_base_dir, \"val/super_resolution_lr\"),\n",
        "    \"val_hr\": os.path.join(reduced_base_dir, \"val/super_resolution_hr\"),\n",
        "    \"test_lr\": os.path.join(reduced_base_dir, \"test/super_resolution_lr\"),\n",
        "    \"test_hr\": os.path.join(reduced_base_dir, \"test/super_resolution_hr\"),\n",
        "}\n",
        "\n",
        "# Create reduced directories\n",
        "for path in reduced_dirs.values():\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Sampling sizes\n",
        "sample_sizes = {\n",
        "    \"train\": 15000,  # Train size\n",
        "    \"val\": 3000,     # Validation size\n",
        "    \"test\": 2000,    # Test size\n",
        "}\n",
        "\n",
        "# Function to sample and copy files\n",
        "def sample_and_copy(original_lr, original_hr, reduced_lr, reduced_hr, num_samples):\n",
        "    # Get sorted file lists\n",
        "    lr_files = sorted(os.listdir(original_lr))\n",
        "    hr_files = sorted(os.listdir(original_hr))\n",
        "\n",
        "    # Ensure the LR and HR file lists match\n",
        "    assert len(lr_files) == len(hr_files), \"Mismatch between LR and HR files!\"\n",
        "\n",
        "    # Randomly sample file indices\n",
        "    sampled_indices = random.sample(range(len(lr_files)), num_samples)\n",
        "\n",
        "    # Copy selected files to reduced directories\n",
        "    for idx in sampled_indices:\n",
        "        shutil.copy(os.path.join(original_lr, lr_files[idx]), reduced_lr)\n",
        "        shutil.copy(os.path.join(original_hr, hr_files[idx]), reduced_hr)\n",
        "\n",
        "# Apply sampling for train, val, and test sets\n",
        "sample_and_copy(\n",
        "    original_dirs[\"train_lr\"], original_dirs[\"train_hr\"],\n",
        "    reduced_dirs[\"train_lr\"], reduced_dirs[\"train_hr\"],\n",
        "    sample_sizes[\"train\"]\n",
        ")\n",
        "\n",
        "sample_and_copy(\n",
        "    original_dirs[\"val_lr\"], original_dirs[\"val_hr\"],\n",
        "    reduced_dirs[\"val_lr\"], reduced_dirs[\"val_hr\"],\n",
        "    sample_sizes[\"val\"]\n",
        ")\n",
        "\n",
        "sample_and_copy(\n",
        "    original_dirs[\"test_lr\"], original_dirs[\"test_hr\"],\n",
        "    reduced_dirs[\"test_lr\"], reduced_dirs[\"test_hr\"],\n",
        "    sample_sizes[\"test\"]\n",
        ")\n",
        "\n",
        "print(\"Reduced dataset created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtTjuqe461Qu",
        "outputId": "291346c5-38a3-49cf-a8f4-006b2898472b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced dataset created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Directories to delete\n",
        "dirs_to_delete = [\n",
        "    \"/content/super_resolution_dataset/test\",\n",
        "    \"/content/super_resolution_dataset/train\",\n",
        "    \"/content/super_resolution_dataset/val\",\n",
        "    \"/content/sample_data\",\n",
        "\n",
        "]\n",
        "\n",
        "# Delete directories\n",
        "for dir_path in dirs_to_delete:\n",
        "    try:\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Deleted: {dir_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Directory not found, skipping: {dir_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting {dir_path}: {e}\")\n",
        "\n",
        "print(\"Selected directories deleted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inO-X1ko6_ZT",
        "outputId": "b6a69455-cb83-467c-da21-704eabf40dc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: /content/super_resolution_dataset/test\n",
            "Deleted: /content/super_resolution_dataset/train\n",
            "Deleted: /content/super_resolution_dataset/val\n",
            "Deleted: /content/sample_data\n",
            "Selected directories deleted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directories for reduced dataset\n",
        "reduced_dirs = {\n",
        "    \"train_lr\": \"/content/super_resolution_dataset/reduced/train/super_resolution_lr\",\n",
        "    \"train_hr\": \"/content/super_resolution_dataset/reduced/train/super_resolution_hr\",\n",
        "    \"val_lr\": \"/content/super_resolution_dataset/reduced/val/super_resolution_lr\",\n",
        "    \"val_hr\": \"/content/super_resolution_dataset/reduced/val/super_resolution_hr\",\n",
        "    \"test_lr\": \"/content/super_resolution_dataset/reduced/test/super_resolution_lr\",\n",
        "    \"test_hr\": \"/content/super_resolution_dataset/reduced/test/super_resolution_hr\",\n",
        "}\n",
        "\n",
        "# Output directories for normalized .npy files\n",
        "normalized_base_dir = \"/content/super_resolution_dataset/normalized_reduced\"\n",
        "normalized_dirs = {key: path.replace(\"reduced\", \"normalized_reduced\") for key, path in reduced_dirs.items()}\n",
        "\n",
        "# Create directories for normalized dataset\n",
        "for path in normalized_dirs.values():\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Normalize images and save as .npy\n",
        "def normalize_and_save(input_dir, output_dir):\n",
        "    for img_name in tqdm(os.listdir(input_dir), desc=f\"Normalizing {input_dir}\"):\n",
        "        input_path = os.path.join(input_dir, img_name)\n",
        "        output_path = os.path.join(output_dir, img_name.replace(\".png\", \".npy\"))\n",
        "\n",
        "        with Image.open(input_path) as img:\n",
        "            img_array = np.asarray(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
        "            img_array = img_array * 2 - 1  # Normalize to [-1, 1]\n",
        "            np.save(output_path, img_array)  # Save as .npy\n",
        "\n",
        "# Apply normalization to all splits\n",
        "for key, input_dir in reduced_dirs.items():\n",
        "    normalize_and_save(input_dir, normalized_dirs[key])\n",
        "\n",
        "print(\"Reduced dataset normalized and saved as .npy!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpGGKeWd68Ly",
        "outputId": "3c2fa43a-a79e-4fa7-bfa2-4fb6685f3feb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalizing /content/super_resolution_dataset/reduced/train/super_resolution_lr: 100%|██████████| 15000/15000 [00:19<00:00, 778.92it/s]\n",
            "Normalizing /content/super_resolution_dataset/reduced/train/super_resolution_hr: 100%|██████████| 15000/15000 [03:17<00:00, 76.10it/s]\n",
            "Normalizing /content/super_resolution_dataset/reduced/val/super_resolution_lr: 100%|██████████| 3000/3000 [00:06<00:00, 476.75it/s]\n",
            "Normalizing /content/super_resolution_dataset/reduced/val/super_resolution_hr: 100%|██████████| 3000/3000 [00:44<00:00, 67.31it/s]\n",
            "Normalizing /content/super_resolution_dataset/reduced/test/super_resolution_lr: 100%|██████████| 2000/2000 [00:04<00:00, 430.44it/s]\n",
            "Normalizing /content/super_resolution_dataset/reduced/test/super_resolution_hr: 100%|██████████| 2000/2000 [00:29<00:00, 68.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced dataset normalized and saved as .npy!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install basicsr\n",
        "import torch\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from basicsr.utils.download_util import load_file_from_url\n",
        "\n",
        "# Initialize the Real-ESRGAN model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = 'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth'  # URL of the pre-trained model\n",
        "\n",
        "# Download the model\n",
        "cached_file = load_file_from_url(\n",
        "    url=model_path, model_dir='/content/realesrgan_models', progress=True, file_name=None\n",
        ")\n",
        "\n",
        "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "model.load_state_dict(torch.load(cached_file)['params_ema'], strict=True)  # Use 'params_ema' for better results\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "ZzC0QONx7D5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SET UP DATALOADER"
      ],
      "metadata": {
        "id": "atI-97So7Qay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define custom dataset for normalized .npy files\n",
        "class SuperResolutionNpyDataset(Dataset):\n",
        "    def __init__(self, lr_dir, hr_dir):\n",
        "        self.lr_dir = lr_dir\n",
        "        self.hr_dir = hr_dir\n",
        "        self.lr_files = sorted(os.listdir(lr_dir))\n",
        "        self.hr_files = sorted(os.listdir(hr_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr_path = os.path.join(self.lr_dir, self.lr_files[idx])\n",
        "        hr_path = os.path.join(self.hr_dir, self.hr_files[idx])\n",
        "\n",
        "        # Load normalized .npy arrays\n",
        "        lr_image = np.load(lr_path)\n",
        "        hr_image = np.load(hr_path)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        lr_tensor = torch.tensor(lr_image).permute(2, 0, 1).float()  # HWC to CHW\n",
        "        hr_tensor = torch.tensor(hr_image).permute(2, 0, 1).float()\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "# Directories for normalized .npy files\n",
        "train_lr_dir = \"/content/super_resolution_dataset/normalized_reduced/train/super_resolution_lr\"\n",
        "train_hr_dir = \"/content/super_resolution_dataset/normalized_reduced/train/super_resolution_hr\"\n",
        "val_lr_dir = \"/content/super_resolution_dataset/normalized_reduced/val/super_resolution_lr\"\n",
        "val_hr_dir = \"/content/super_resolution_dataset/normalized_reduced/val/super_resolution_hr\"\n",
        "test_lr_dir = \"/content/super_resolution_dataset/normalized_reduced/test/super_resolution_lr\"\n",
        "test_hr_dir = \"/content/super_resolution_dataset/normalized_reduced/test/super_resolution_hr\"\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "batch_size = 16  # Adjust batch size if needed\n",
        "train_dataset = SuperResolutionNpyDataset(train_lr_dir, train_hr_dir)\n",
        "val_dataset = SuperResolutionNpyDataset(val_lr_dir, val_hr_dir)\n",
        "test_dataset = SuperResolutionNpyDataset(test_lr_dir, test_hr_dir)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Verify dataloader\n",
        "for lr_batch, hr_batch in train_loader:\n",
        "    print(f\"LR Batch Shape: {lr_batch.shape}, Range: {lr_batch.min().item()} to {lr_batch.max().item()}\")\n",
        "    print(f\"HR Batch Shape: {hr_batch.shape}, Range: {hr_batch.min().item()} to {hr_batch.max().item()}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9SJ-k9L7N3d",
        "outputId": "77b03947-2c88-4945-aec8-c1beef15a775"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Batch Shape: torch.Size([16, 3, 128, 128]), Range: -1.0 to 1.0\n",
            "HR Batch Shape: torch.Size([16, 3, 512, 512]), Range: -1.0 to 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Simulated LR image\n",
        "with torch.no_grad():\n",
        "    output = model(dummy_input)\n",
        "print(f\"Output Shape: {output.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVj9gO7SM6sX",
        "outputId": "dad773fc-c4c9-4785-a17c-e12392042d50"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
      ],
      "metadata": {
        "id": "0Qnfia-HFB0e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()  # Clears cached memory\n"
      ],
      "metadata": {
        "id": "wNVJZd2rPTJx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # Avoid splitting large memory blocks\n"
      ],
      "metadata": {
        "id": "MOo1VNunPUfY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "OYTxYZdPPY46"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast  # Import for mixed precision training\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.L1Loss()  # Pixel-wise loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate as needed\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Optional learning rate scheduler\n",
        "scaler = GradScaler()  # Mixed precision scaler\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "patience = 4  # Early stopping patience\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "accumulation_steps = 2  # Gradient accumulation steps\n",
        "batch_size = 4  # Adjust if memory issues persist\n",
        "\n",
        "# TensorBoard for logging\n",
        "writer = SummaryWriter(log_dir=\"/content/runs/reduced_dataset_training\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    total_batches = len(train_loader)\n",
        "\n",
        "    # Train\n",
        "    for i, (lr_batch, hr_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "        lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
        "\n",
        "        # Mixed precision training\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():  # Forward pass in mixed precision\n",
        "            sr_batch = model(lr_batch)\n",
        "            loss = criterion(sr_batch, hr_batch)\n",
        "\n",
        "        # Backward pass with scaled loss\n",
        "        loss = loss / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)  # Scaled optimizer step\n",
        "            scaler.update()  # Update the scaler\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item() * accumulation_steps  # Accumulated loss\n",
        "\n",
        "        # Log training progress\n",
        "        writer.add_scalar(\"Batch Loss/Train\", loss.item() * accumulation_steps, epoch * total_batches + i)\n",
        "\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "    writer.add_scalar(\"Loss/Train\", train_loss, epoch + 1)\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {train_loss:.6f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_val_batches = len(val_loader)\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_batch, hr_batch) in enumerate(tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\")):\n",
        "            lr_batch, hr_batch = lr_batch.to(device), hr_batch.to(device)\n",
        "\n",
        "            with autocast():  # Mixed precision during validation\n",
        "                sr_batch = model(lr_batch)\n",
        "                loss = criterion(sr_batch, hr_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Log validation progress\n",
        "            writer.add_scalar(\"Batch Loss/Validation\", loss.item(), epoch * total_val_batches + i)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch + 1)\n",
        "    print(f\"Epoch {epoch+1} Validation Loss: {val_loss:.6f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        # Save best model\n",
        "        best_model_path = f\"/content/drive/My Drive/best_model_epoch_{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved best model at epoch {epoch+1} to {best_model_path}\")\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "\n",
        "    if early_stop_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    scheduler.step()  # Adjust learning rate\n",
        "\n",
        "writer.close()\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W1hgXX2PenW",
        "outputId": "02ff8cae-8d2b-426c-d2cb-f99c539f31bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-6eb08c16e983>:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Mixed precision scaler\n",
            "Epoch 1/20:   0%|          | 0/938 [00:00<?, ?it/s]<ipython-input-22-6eb08c16e983>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Forward pass in mixed precision\n",
            "Epoch 1/20: 100%|██████████| 938/938 [11:56<00:00,  1.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Training Loss: 0.337367\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 1/20:   0%|          | 0/188 [00:00<?, ?it/s]<ipython-input-22-6eb08c16e983>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Mixed precision during validation\n",
            "Validation 1/20: 100%|██████████| 188/188 [01:32<00:00,  2.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Validation Loss: 0.335613\n",
            "Saved best model at epoch 1 to /content/drive/My Drive/best_model_epoch_1.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 938/938 [11:13<00:00,  1.39it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Training Loss: 0.330349\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 2/20: 100%|██████████| 188/188 [01:32<00:00,  2.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Validation Loss: 0.329696\n",
            "Saved best model at epoch 2 to /content/drive/My Drive/best_model_epoch_2.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 938/938 [11:32<00:00,  1.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Training Loss: 0.326577\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 3/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Validation Loss: 0.326942\n",
            "Saved best model at epoch 3 to /content/drive/My Drive/best_model_epoch_3.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 938/938 [11:01<00:00,  1.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Training Loss: 0.324594\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 4/20: 100%|██████████| 188/188 [01:15<00:00,  2.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Validation Loss: 0.326245\n",
            "Saved best model at epoch 4 to /content/drive/My Drive/best_model_epoch_4.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 938/938 [10:03<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Training Loss: 0.322063\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 5/20: 100%|██████████| 188/188 [00:48<00:00,  3.87it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Validation Loss: 0.320180\n",
            "Saved best model at epoch 5 to /content/drive/My Drive/best_model_epoch_5.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 938/938 [09:50<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Training Loss: 0.321137\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 6/20: 100%|██████████| 188/188 [00:48<00:00,  3.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Validation Loss: 0.319874\n",
            "Saved best model at epoch 6 to /content/drive/My Drive/best_model_epoch_6.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Training Loss: 0.320080\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 7/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Validation Loss: 0.321622\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Training Loss: 0.318643\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 8/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Validation Loss: 0.318118\n",
            "Saved best model at epoch 8 to /content/drive/My Drive/best_model_epoch_8.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 938/938 [09:50<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Training Loss: 0.318154\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 9/20: 100%|██████████| 188/188 [00:48<00:00,  3.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Validation Loss: 0.317494\n",
            "Saved best model at epoch 9 to /content/drive/My Drive/best_model_epoch_9.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Training Loss: 0.317131\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 10/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Validation Loss: 0.317027\n",
            "Saved best model at epoch 10 to /content/drive/My Drive/best_model_epoch_10.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 Training Loss: 0.313009\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 11/20: 100%|██████████| 188/188 [00:48<00:00,  3.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 Validation Loss: 0.315211\n",
            "Saved best model at epoch 11 to /content/drive/My Drive/best_model_epoch_11.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 Training Loss: 0.311723\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 12/20: 100%|██████████| 188/188 [00:48<00:00,  3.87it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 Validation Loss: 0.315118\n",
            "Saved best model at epoch 12 to /content/drive/My Drive/best_model_epoch_12.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 Training Loss: 0.310869\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 13/20: 100%|██████████| 188/188 [00:48<00:00,  3.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 Validation Loss: 0.318512\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 Training Loss: 0.310755\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 14/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 Validation Loss: 0.316483\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 Training Loss: 0.309614\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 15/20: 100%|██████████| 188/188 [00:48<00:00,  3.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 Validation Loss: 0.314180\n",
            "Saved best model at epoch 15 to /content/drive/My Drive/best_model_epoch_15.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 Training Loss: 0.308963\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 16/20: 100%|██████████| 188/188 [00:48<00:00,  3.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 Validation Loss: 0.315194\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 938/938 [09:49<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 Training Loss: 0.308346\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 17/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 Validation Loss: 0.317538\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 938/938 [09:50<00:00,  1.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 Training Loss: 0.308292\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation 18/20: 100%|██████████| 188/188 [00:48<00:00,  3.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 Validation Loss: 0.316802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20:  13%|█▎        | 125/938 [01:18<08:29,  1.59it/s]"
          ]
        }
      ]
    }
  ]
}