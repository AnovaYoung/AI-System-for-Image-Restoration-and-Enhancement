{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnovaYoung/AI-System-for-Image-Restoration-and-Enhancement/blob/Data-Cleaning-and-Preparation/Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoWFy_zhwBZr",
        "outputId": "5c4f1767-a9b6-42f7-b9a3-43d059219d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Zip file found! Extracting...\n",
            "Extraction complete!\n",
            "Number of files in extracted dataset: 4\n",
            "Sample directories and files:\n",
            "Directory: /content/unified_dataset_extracted, Files: []\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the zip file in Google Drive\n",
        "zip_file_path = \"/content/drive/My Drive/unified_dataset.zip\"\n",
        "\n",
        "# Directory to extract the zip file\n",
        "extraction_dir = \"/content/unified_dataset_extracted\"\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(zip_file_path):\n",
        "    print(\"Zip file found! Extracting...\")\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_dir)\n",
        "    print(\"Extraction complete!\")\n",
        "\n",
        "    # Verify contents\n",
        "    print(f\"Number of files in extracted dataset: {len(os.listdir(extraction_dir))}\")\n",
        "    print(\"Sample directories and files:\")\n",
        "    for root, dirs, files in os.walk(extraction_dir):\n",
        "        print(f\"Directory: {root}, Files: {files[:5]}\")\n",
        "        break\n",
        "else:\n",
        "    print(\"Zip file not found. Please check the path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNBi_mv8Ba42"
      },
      "source": [
        "I believe the way I structured this was that the Directory holds other Directories and the root files are within those, so lets explore further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYis7g1qBIoI",
        "outputId": "acc22612-006f-428e-a2d6-43f667db345c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory: /content/unified_dataset_extracted\n",
            "Subdirectories: ['super_resolution_hr', 'data', 'super_resolution_lr', 'content']\n",
            "Files: []\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/super_resolution_hr\n",
            "Subdirectories: []\n",
            "Files: ['0482.img', '0079.img', '0200.img', '0426.img', '0189.img']\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/data\n",
            "Subdirectories: ['cifar']\n",
            "Files: []\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/super_resolution_lr\n",
            "Subdirectories: []\n",
            "Files: ['0482.img', '0079.img', '0200.img', '0426.img', '0189.img']\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/content\n",
            "Subdirectories: ['data', 'unified_dataset_extracted']\n",
            "Files: []\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Function to list files and subdirectories in the dataset\n",
        "def explore_directory(directory, depth=1):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        # Limit the exploration depth to avoid unnecessary details\n",
        "        current_depth = root[len(directory):].count(os.sep)\n",
        "        if current_depth < depth:\n",
        "            print(f\"Directory: {root}\")\n",
        "            print(f\"Subdirectories: {dirs[:5]}\")  # Show a sample of subdirectories\n",
        "            print(f\"Files: {files[:5]}\")  # Show a sample of files\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "# Explore the extracted directory\n",
        "explore_directory(extraction_dir, depth=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "61fp3Rc9HJwO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the `/content` directory within the extracted dataset\n",
        "content_dir_path = \"/content/unified_dataset_extracted/content\"\n",
        "\n",
        "# Traverse the `/content` directory to analyze its structure\n",
        "for root, dirs, files in os.walk(content_dir_path):\n",
        "    print(f\"Directory: {root}\")\n",
        "    print(f\"Subdirectories: {dirs}\")\n",
        "    print(f\"Files: {files}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/unified_dataset_extracted\"\n",
        "\n",
        "# List to store paths of .txt files\n",
        "txt_files = []\n",
        "\n",
        "# Traverse the directory structure to find all .txt files\n",
        "for root, _, files in os.walk(base_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            txt_files.append(os.path.join(root, file))\n",
        "\n",
        "# Print the total count of .txt files found\n",
        "print(f\"Total .txt files found: {len(txt_files)}\")\n",
        "\n",
        "# Print first 10 .txt files as a sample\n",
        "print(\"Sample .txt file paths:\")\n",
        "print(txt_files[:10])\n",
        "\n",
        "# Function to display content of a .txt file\n",
        "def preview_txt_file(file_path, num_lines=10):\n",
        "    print(f\"\\nContents of {file_path}:\")\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= num_lines:\n",
        "                    break\n",
        "                print(line.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "# Preview content of the first few .txt files\n",
        "for file_path in txt_files[:5]:\n",
        "    preview_txt_file(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYmMBkPbISTs",
        "outputId": "96970cd1-dfd4-477b-f681-a781b7428856"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total .txt files found: 812\n",
            "Sample .txt file paths:\n",
            "['/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/wnids.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/words.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/val/val_annotations.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/wnids.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/words.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/val/val_annotations.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/train/n01768244/n01768244_boxes.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/train/n02002724/n02002724_boxes.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/train/n04356056/n04356056_boxes.txt', '/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/train/n02892201/n02892201_boxes.txt']\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/wnids.txt:\n",
            "n02124075\n",
            "n04067472\n",
            "n04540053\n",
            "n04099969\n",
            "n07749582\n",
            "n01641577\n",
            "n02802426\n",
            "n09246464\n",
            "n07920052\n",
            "n03970156\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/words.txt:\n",
            "n00001740\tentity\n",
            "n00001930\tphysical entity\n",
            "n00002137\tabstraction, abstract entity\n",
            "n00002452\tthing\n",
            "n00002684\tobject, physical object\n",
            "n00003553\twhole, unit\n",
            "n00003993\tcongener\n",
            "n00004258\tliving thing, animate thing\n",
            "n00004475\torganism, being\n",
            "n00005787\tbenthos\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/val/val_annotations.txt:\n",
            "val_0.JPEG\tn03444034\t0\t32\t44\t62\n",
            "val_1.JPEG\tn04067472\t52\t55\t57\t59\n",
            "val_2.JPEG\tn04070727\t4\t0\t60\t55\n",
            "val_3.JPEG\tn02808440\t3\t3\t63\t63\n",
            "val_4.JPEG\tn02808440\t9\t27\t63\t48\n",
            "val_5.JPEG\tn04399382\t7\t0\t59\t63\n",
            "val_6.JPEG\tn04179913\t0\t0\t63\t56\n",
            "val_7.JPEG\tn02823428\t5\t0\t57\t63\n",
            "val_8.JPEG\tn04146614\t0\t31\t60\t60\n",
            "val_9.JPEG\tn02226429\t0\t3\t63\t57\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/wnids.txt:\n",
            "n02124075\n",
            "n04067472\n",
            "n04540053\n",
            "n04099969\n",
            "n07749582\n",
            "n01641577\n",
            "n02802426\n",
            "n09246464\n",
            "n07920052\n",
            "n03970156\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/words.txt:\n",
            "n00001740\tentity\n",
            "n00001930\tphysical entity\n",
            "n00002137\tabstraction, abstract entity\n",
            "n00002452\tthing\n",
            "n00002684\tobject, physical object\n",
            "n00003553\twhole, unit\n",
            "n00003993\tcongener\n",
            "n00004258\tliving thing, animate thing\n",
            "n00004475\torganism, being\n",
            "n00005787\tbenthos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breakdown of .txt File\n",
        "\n",
        "1. **wnids.txt**\n",
        "\n",
        "Content: A list of WordNet IDs (wnid), likely corresponding to the categories or labels in the Tiny ImageNet dataset.\n",
        "\n",
        "Use: These IDs map to specific object categories.\n",
        "\n",
        "Example Content:\n",
        "\n",
        "n02124075\n",
        "\n",
        "n04067472\n",
        "\n",
        "n04540053\n",
        "...\n",
        "2. **words.txt**\n",
        "\n",
        "Content: A mapping of WordNet IDs (wnid) to their respective English labels or descriptions.\n",
        "\n",
        "Use: Provides a human-readable explanation for the categories in wnids.txt.\n",
        "\n",
        "Example Content:\n",
        "\n",
        "n00001740\tentity\n",
        "\n",
        "n00001930\tphysical entity\n",
        "\n",
        "n00002137\tabstraction, abstract entity\n",
        "...\n",
        "3. **val_annotations.txt**\n",
        "\n",
        "Content: Annotation data for the validation set, including:\n",
        "1. Image name\n",
        "2. Category ID (wnid)\n",
        "3. Bounding box coordinates (x1, y1, x2, y2)\n",
        "\n",
        "Use: Useful for tasks like object localization or evaluation.\n",
        "\n",
        "Example Content:\n",
        "\n",
        "val_0.JPEG\tn03444034\t0\t32\t44\t62\n",
        "\n",
        "val_1.JPEG\tn04067472\t52\t55\t57\t59\n",
        "\n",
        "...\n",
        "\n",
        "Summary of .txt File Roles in Tiny ImageNet:\n",
        "\n",
        "**wnids.txt:** Lists the categories available.\n",
        "\n",
        "**words.txt**: Maps category IDs to descriptions.\n",
        "\n",
        "**val_annotations.txt**: Provides validation image metadata, including bounding boxes.\n"
      ],
      "metadata": {
        "id": "IPVb2wvEItXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore /content/unified_dataset_extracted/content for duplicate or redundant data."
      ],
      "metadata": {
        "id": "sdDyl6a3K8pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def explore_content_dir(base_dir):\n",
        "    print(f\"Exploring: {base_dir}\")\n",
        "    dir_structure = defaultdict(list)\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        relative_root = os.path.relpath(root, base_dir)\n",
        "        if relative_root != \".\":\n",
        "            dir_structure[relative_root] = dirs + files\n",
        "\n",
        "    for dir_path, items in dir_structure.items():\n",
        "        print(f\"Directory: {dir_path}\")\n",
        "        print(f\"Contents ({len(items)} items): {', '.join(items[:10])}{'...' if len(items) > 10 else ''}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "content_dir = \"/content/unified_dataset_extracted/content\"\n",
        "explore_content_dir(content_dir)\n"
      ],
      "metadata": {
        "id": "M-Et2XbqKYGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check CIFAR Directory"
      ],
      "metadata": {
        "id": "vT9o75c5K2Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_cifar_structure(cifar_dir):\n",
        "    print(f\"Exploring: {cifar_dir}\")\n",
        "    for root, dirs, files in os.walk(cifar_dir):\n",
        "        print(f\"Directory: {root}\")\n",
        "        print(f\"Subdirectories: {dirs}\")\n",
        "        print(f\"Files ({len(files)}): {files[:10]}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "cifar_dir = \"/content/unified_dataset_extracted/data/cifar\"\n",
        "explore_cifar_structure(cifar_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd_0VeQOKstF",
        "outputId": "867168c5-46fd-46dd-bb20-9de1f94f471c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring: /content/unified_dataset_extracted/data/cifar\n",
            "Directory: /content/unified_dataset_extracted/data/cifar\n",
            "Subdirectories: ['cifar10_lr', 'cifar10_hr', 'cifar100_lr', 'cifar100_hr']\n",
            "Files (0): []\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/data/cifar/cifar10_lr\n",
            "Subdirectories: []\n",
            "Files (50000): ['23095_lr.img', '601_lr.img', '1158_lr.img', '1577_lr.img', '34179_lr.img', '34011_lr.img', '45339_lr.img', '13748_lr.img', '31112_lr.img', '4361_lr.img']\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/data/cifar/cifar10_hr\n",
            "Subdirectories: []\n",
            "Files (50000): ['1373_hr.img', '34796_hr.img', '29068_hr.img', '40732_hr.img', '15178_hr.img', '44114_hr.img', '30262_hr.img', '43943_hr.img', '36923_hr.img', '6911_hr.img']\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/data/cifar/cifar100_lr\n",
            "Subdirectories: []\n",
            "Files (50000): ['23095_lr.img', '601_lr.img', '1158_lr.img', '1577_lr.img', '34179_lr.img', '34011_lr.img', '45339_lr.img', '13748_lr.img', '31112_lr.img', '4361_lr.img']\n",
            "----------------------------------------\n",
            "Directory: /content/unified_dataset_extracted/data/cifar/cifar100_hr\n",
            "Subdirectories: []\n",
            "Files (50000): ['1373_hr.img', '34796_hr.img', '29068_hr.img', '40732_hr.img', '15178_hr.img', '44114_hr.img', '30262_hr.img', '43943_hr.img', '36923_hr.img', '6911_hr.img']\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify images align with intended tasks"
      ],
      "metadata": {
        "id": "4xoopdLOLIhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def verify_images(base_dir, task_dirs):\n",
        "    for task_dir in task_dirs:\n",
        "        task_path = os.path.join(base_dir, task_dir)\n",
        "        print(f\"Verifying images in {task_dir}...\")\n",
        "        for img_file in os.listdir(task_path)[:10]:  # Checking a few images\n",
        "            img_path = os.path.join(task_path, img_file)\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    print(f\"Image {img_file}: {img.size}, {img.mode}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with file {img_file}: {e}\")\n",
        "\n",
        "task_base = \"/content/unified_dataset_extracted\"\n",
        "tasks = [\"super_resolution_hr\", \"super_resolution_lr\"]\n",
        "verify_images(task_base, tasks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6iiY4FsK0n7",
        "outputId": "625cef50-c496-40fd-d33f-d242721f3343"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying images in super_resolution_hr...\n",
            "Image 0482.img: (2040, 1488), RGB\n",
            "Image 0079.img: (2040, 1356), RGB\n",
            "Image 0200.img: (2040, 1356), RGB\n",
            "Image 0426.img: (2040, 1164), RGB\n",
            "Image 0189.img: (2040, 1356), RGB\n",
            "Image 0081.img: (2040, 1356), RGB\n",
            "Image 0342.img: (2040, 1356), RGB\n",
            "Image 0346.img: (2040, 1356), RGB\n",
            "Image 0796.img: (2040, 1356), RGB\n",
            "Image 0631.img: (2040, 1356), RGB\n",
            "Verifying images in super_resolution_lr...\n",
            "Image 0482.img: (510, 372), RGB\n",
            "Image 0079.img: (510, 339), RGB\n",
            "Image 0200.img: (510, 339), RGB\n",
            "Image 0426.img: (510, 291), RGB\n",
            "Image 0189.img: (510, 339), RGB\n",
            "Image 0081.img: (510, 339), RGB\n",
            "Image 0342.img: (510, 339), RGB\n",
            "Image 0346.img: (510, 339), RGB\n",
            "Image 0796.img: (510, 339), RGB\n",
            "Image 0631.img: (510, 339), RGB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def clean_redundancy(base_dir, save_space=False):\n",
        "    unique_dirs = set()\n",
        "    duplicates = []\n",
        "\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        dir_tuple = tuple(sorted(dirs + files))\n",
        "        if dir_tuple in unique_dirs:\n",
        "            duplicates.append(root)\n",
        "        else:\n",
        "            unique_dirs.add(dir_tuple)\n",
        "\n",
        "    print(f\"Found {len(duplicates)} duplicate directories.\")\n",
        "    if save_space:\n",
        "        for dup in duplicates:\n",
        "            shutil.rmtree(dup)\n",
        "            print(f\"Removed duplicate: {dup}\")\n",
        "\n",
        "content_dir = \"/content/unified_dataset_extracted/content\"\n",
        "clean_redundancy(content_dir, save_space=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV132TFJLVge",
        "outputId": "34f70392-172e-4ec2-9184-8882cd088af8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1421 duplicate directories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "**Handle Duplicates:**\n",
        "\n",
        "Automate the removal of duplicate directories and verify integrity.\n",
        "\n",
        "**Align Low-Res and High-Res Image Pairs:**\n",
        "\n",
        "Check if each low-res image in the CIFAR and super-resolution datasets has a corresponding high-res image.\n",
        "\n",
        "**Metadata Exploration:**\n",
        "\n",
        "Investigate .txt files (_boxes.txt) to confirm their utility for tasks like inpainting or bounding box annotations.\n"
      ],
      "metadata": {
        "id": "q2HXFqQPLzBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Remove Duplicate Directories and Validate Integrity"
      ],
      "metadata": {
        "id": "QKJPmqCLMARz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from filecmp import dircmp\n",
        "\n",
        "# Base directory\n",
        "base_dir = \"/content/unified_dataset_extracted/content\"\n",
        "\n",
        "# Function to compare and remove duplicate directories\n",
        "def remove_duplicates(base_dir):\n",
        "    seen_directories = {}\n",
        "    duplicates_removed = 0\n",
        "\n",
        "    for root, dirs, _ in os.walk(base_dir):\n",
        "        for dir_name in dirs:\n",
        "            dir_path = os.path.join(root, dir_name)\n",
        "            # Check if this directory is identical to one we've already seen\n",
        "            for seen_dir, seen_path in seen_directories.items():\n",
        "                comparison = dircmp(dir_path, seen_path)\n",
        "                if not comparison.left_only and not comparison.right_only and not comparison.diff_files:\n",
        "                    print(f\"Duplicate found: {dir_path} matches {seen_path}. Removing duplicate.\")\n",
        "                    shutil.rmtree(dir_path)\n",
        "                    duplicates_removed += 1\n",
        "                    break\n",
        "            else:\n",
        "                seen_directories[dir_name] = dir_path  # Add new unique directory\n",
        "\n",
        "    print(f\"Total duplicates removed: {duplicates_removed}\")\n",
        "\n",
        "# Execute duplicate removal\n",
        "remove_duplicates(base_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZYSlHGiMBxM",
        "outputId": "955aa090-e113-4588-db0b-f243741c094f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate found: /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/val/organized matches /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/train. Removing duplicate.\n",
            "Duplicate found: /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/test matches /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/test. Removing duplicate.\n",
            "Duplicate found: /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/val matches /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/val. Removing duplicate.\n",
            "Duplicate found: /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/train matches /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/train. Removing duplicate.\n",
            "Duplicate found: /content/unified_dataset_extracted/content/unified_dataset_extracted/content/data matches /content/unified_dataset_extracted/content/data. Removing duplicate.\n",
            "Total duplicates removed: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Verify Alignment of Low-Res and High-Res Image Pairs"
      ],
      "metadata": {
        "id": "vFw-to1bMgQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Paths for CIFAR and super-resolution datasets\n",
        "cifar_paths = [\"/content/unified_dataset_extracted/data/cifar/cifar10_lr\",\n",
        "               \"/content/unified_dataset_extracted/data/cifar/cifar10_hr\",\n",
        "               \"/content/unified_dataset_extracted/data/cifar/cifar100_lr\",\n",
        "               \"/content/unified_dataset_extracted/data/cifar/cifar100_hr\"]\n",
        "\n",
        "super_res_paths = [\"/content/unified_dataset_extracted/super_resolution_lr\",\n",
        "                   \"/content/unified_dataset_extracted/super_resolution_hr\"]\n",
        "\n",
        "def check_alignment(low_res_dir, high_res_dir):\n",
        "    low_res_files = set(os.path.basename(f) for f in glob.glob(f\"{low_res_dir}/*.img\"))\n",
        "    high_res_files = set(os.path.basename(f) for f in glob.glob(f\"{high_res_dir}/*.img\"))\n",
        "\n",
        "    # Match files\n",
        "    missing_in_high_res = low_res_files - high_res_files\n",
        "    missing_in_low_res = high_res_files - low_res_files\n",
        "\n",
        "    print(f\"Checking alignment between {low_res_dir} and {high_res_dir}\")\n",
        "    print(f\"Missing in high-res: {len(missing_in_high_res)}\")\n",
        "    print(f\"Missing in low-res: {len(missing_in_low_res)}\")\n",
        "\n",
        "    if missing_in_high_res:\n",
        "        print(f\"Sample missing in high-res: {list(missing_in_high_res)[:5]}\")\n",
        "    if missing_in_low_res:\n",
        "        print(f\"Sample missing in low-res: {list(missing_in_low_res)[:5]}\")\n",
        "\n",
        "# Verify CIFAR datasets\n",
        "for i in range(0, len(cifar_paths), 2):\n",
        "    check_alignment(cifar_paths[i], cifar_paths[i + 1])\n",
        "\n",
        "# Verify super-resolution datasets\n",
        "check_alignment(super_res_paths[0], super_res_paths[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5N1DfLBMjLl",
        "outputId": "fa4254ad-8351-4e52-8fcc-3cb4abab0bb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking alignment between /content/unified_dataset_extracted/data/cifar/cifar10_lr and /content/unified_dataset_extracted/data/cifar/cifar10_hr\n",
            "Missing in high-res: 50000\n",
            "Missing in low-res: 50000\n",
            "Sample missing in high-res: ['3401_lr.img', '39615_lr.img', '3560_lr.img', '11408_lr.img', '37560_lr.img']\n",
            "Sample missing in low-res: ['3532_hr.img', '5586_hr.img', '29499_hr.img', '16280_hr.img', '39747_hr.img']\n",
            "Checking alignment between /content/unified_dataset_extracted/data/cifar/cifar100_lr and /content/unified_dataset_extracted/data/cifar/cifar100_hr\n",
            "Missing in high-res: 50000\n",
            "Missing in low-res: 50000\n",
            "Sample missing in high-res: ['3401_lr.img', '39615_lr.img', '3560_lr.img', '11408_lr.img', '37560_lr.img']\n",
            "Sample missing in low-res: ['3532_hr.img', '5586_hr.img', '29499_hr.img', '16280_hr.img', '39747_hr.img']\n",
            "Checking alignment between /content/unified_dataset_extracted/super_resolution_lr and /content/unified_dataset_extracted/super_resolution_hr\n",
            "Missing in high-res: 0\n",
            "Missing in low-res: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_base_dir = \"/content/unified_dataset_extracted\"\n",
        "\n",
        "def analyze_txt_files(base_dir):\n",
        "    txt_files = glob.glob(f\"{base_dir}/**/*.txt\", recursive=True)\n",
        "    print(f\"Total .txt files found: {len(txt_files)}\")\n",
        "\n",
        "    for file_path in txt_files[:5]:  # Display the first 5 files as samples\n",
        "        print(f\"\\nContents of {file_path}:\")\n",
        "        try:\n",
        "            with open(file_path, \"r\") as f:\n",
        "                content = f.readlines()\n",
        "                print(\"\".join(content[:10]))  # Show the first 10 lines\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "analyze_txt_files(txt_base_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGekMo7lNwXD",
        "outputId": "f479f5e2-c82c-4ff6-ec89-9d04d330f71f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total .txt files found: 205\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/wnids.txt:\n",
            "n02124075\n",
            "n04067472\n",
            "n04540053\n",
            "n04099969\n",
            "n07749582\n",
            "n01641577\n",
            "n02802426\n",
            "n09246464\n",
            "n07920052\n",
            "n03970156\n",
            "\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/words.txt:\n",
            "n00001740\tentity\n",
            "n00001930\tphysical entity\n",
            "n00002137\tabstraction, abstract entity\n",
            "n00002452\tthing\n",
            "n00002684\tobject, physical object\n",
            "n00003553\twhole, unit\n",
            "n00003993\tcongener\n",
            "n00004258\tliving thing, animate thing\n",
            "n00004475\torganism, being\n",
            "n00005787\tbenthos\n",
            "\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/val/val_annotations.txt:\n",
            "val_0.JPEG\tn03444034\t0\t32\t44\t62\n",
            "val_1.JPEG\tn04067472\t52\t55\t57\t59\n",
            "val_2.JPEG\tn04070727\t4\t0\t60\t55\n",
            "val_3.JPEG\tn02808440\t3\t3\t63\t63\n",
            "val_4.JPEG\tn02808440\t9\t27\t63\t48\n",
            "val_5.JPEG\tn04399382\t7\t0\t59\t63\n",
            "val_6.JPEG\tn04179913\t0\t0\t63\t56\n",
            "val_7.JPEG\tn02823428\t5\t0\t57\t63\n",
            "val_8.JPEG\tn04146614\t0\t31\t60\t60\n",
            "val_9.JPEG\tn02226429\t0\t3\t63\t57\n",
            "\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/wnids.txt:\n",
            "n02124075\n",
            "n04067472\n",
            "n04540053\n",
            "n04099969\n",
            "n07749582\n",
            "n01641577\n",
            "n02802426\n",
            "n09246464\n",
            "n07920052\n",
            "n03970156\n",
            "\n",
            "\n",
            "Contents of /content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200/words.txt:\n",
            "n00001740\tentity\n",
            "n00001930\tphysical entity\n",
            "n00002137\tabstraction, abstract entity\n",
            "n00002452\tthing\n",
            "n00002684\tobject, physical object\n",
            "n00003553\twhole, unit\n",
            "n00003993\tcongener\n",
            "n00004258\tliving thing, animate thing\n",
            "n00004475\torganism, being\n",
            "n00005787\tbenthos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing Alignment Between Low-Res and High-Res for CIFAR:**\n",
        "\n",
        "Both cifar10 and cifar100 exhibit a complete mismatch between their lr (low-resolution) and hr (high-resolution) datasets.\n",
        "\n",
        "\n",
        "# Proposed Plan:\n",
        "\n",
        "Select CIFAR-100 for Refinement:\n",
        "\n",
        "Since CIFAR-100 contains more classes (100 vs. 10 in CIFAR-10), it's a richer dataset for demonstrating resolution-based image restoration tasks.\n",
        "Let's focus on aligning HR and LR images for CIFAR-100.\n",
        "\n",
        "Generate LR Images from HR:\n",
        "\n",
        "Use the HR versions as the source dataset.\n",
        "Downscale the HR images to a lower resolution programmatically, creating new matching LR counterparts.\n",
        "\n",
        "Potentially drop CIFAR-10.\n"
      ],
      "metadata": {
        "id": "K2dsGn05O2vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "hr_dir = \"/content/unified_dataset_extracted/super_resolution_hr\"\n",
        "lr_dir = \"/content/unified_dataset_extracted/super_resolution_lr\"\n",
        "os.makedirs(hr_dir, exist_ok=True)\n",
        "os.makedirs(lr_dir, exist_ok=True)\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "(x_train, _), (x_test, _) = cifar100.load_data()\n",
        "x_data = np.concatenate([x_train, x_test], axis=0)\n",
        "\n",
        "# Select 50,000 for training and 10,000 for testing\n",
        "x_train, x_test = train_test_split(x_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure the counts align with your specifications\n",
        "x_train = x_train[:50000]\n",
        "x_test = x_test[:10000]\n",
        "\n",
        "# Save high-resolution and corresponding low-resolution images\n",
        "def save_images(images, hr_dir, lr_dir, start_idx=0):\n",
        "    for i, img in enumerate(images):\n",
        "        # Save high-resolution image\n",
        "        hr_path = os.path.join(hr_dir, f\"img_{start_idx + i:05d}_hr.png\")\n",
        "        Image.fromarray(img).save(hr_path)\n",
        "\n",
        "        # Create and save low-resolution image\n",
        "        lr_img = Image.fromarray(img).resize((16, 16), Image.BICUBIC).resize((32, 32), Image.BICUBIC)\n",
        "        lr_path = os.path.join(lr_dir, f\"img_{start_idx + i:05d}_lr.png\")\n",
        "        lr_img.save(lr_path)\n",
        "\n",
        "    print(f\"Saved {len(images)} high-resolution and low-resolution pairs.\")\n",
        "\n",
        "# Save training and testing pairs\n",
        "save_images(x_train, hr_dir, lr_dir, start_idx=0)\n",
        "save_images(x_test, hr_dir, lr_dir, start_idx=50000)\n",
        "\n",
        "print(\"CIFAR-100 dataset preparation complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc1RfnGFPJdX",
        "outputId": "b44098d8-009e-4f18-d813-f49049854220"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 48000 high-resolution and low-resolution pairs.\n",
            "Saved 10000 high-resolution and low-resolution pairs.\n",
            "CIFAR-100 dataset preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERIFY BEFORE CONCATANATION"
      ],
      "metadata": {
        "id": "7TUL2ZdMUhqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total files in HR directory: {len(hr_files)}\")\n",
        "print(f\"Total files in LR directory: {len(lr_files)}\")\n",
        "\n",
        "# Split train and test by filename convention\n",
        "train_hr_files = [f for f in hr_files if \"train\" in f]\n",
        "train_lr_files = [f for f in lr_files if \"train\" in f]\n",
        "test_hr_files = [f for f in hr_files if \"test\" in f]\n",
        "test_lr_files = [f for f in lr_files if \"test\" in f]\n",
        "\n",
        "print(f\"Training HR files: {len(train_hr_files)}\")\n",
        "print(f\"Training LR files: {len(train_lr_files)}\")\n",
        "print(f\"Test HR files: {len(test_hr_files)}\")\n",
        "print(f\"Test LR files: {len(test_lr_files)}\")\n",
        "\n",
        "# Check for missing files\n",
        "missing_in_lr = [hr.replace(\"_hr\", \"_lr\") for hr in train_hr_files if hr.replace(\"_hr\", \"_lr\") not in train_lr_files]\n",
        "missing_in_hr = [lr.replace(\"_lr\", \"_hr\") for lr in train_lr_files if lr.replace(\"_lr\", \"_hr\") not in train_hr_files]\n",
        "\n",
        "# Print missing files for training\n",
        "print(f\"Missing in LR (train): {len(missing_in_lr)} files\")\n",
        "print(f\"Missing in HR (train): {len(missing_in_hr)} files\")\n",
        "\n",
        "# Repeat for test files\n",
        "missing_in_lr_test = [hr.replace(\"_hr\", \"_lr\") for hr in test_hr_files if hr.replace(\"_hr\", \"_lr\") not in test_lr_files]\n",
        "missing_in_hr_test = [lr.replace(\"_lr\", \"_hr\") for lr in test_lr_files if lr.replace(\"_lr\", \"_hr\") not in test_hr_files]\n",
        "\n",
        "# Print missing files for testing\n",
        "print(f\"Missing in LR (test): {len(missing_in_lr_test)} files\")\n",
        "print(f\"Missing in HR (test): {len(missing_in_hr_test)} files\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJueMamvVdSo",
        "outputId": "17a43068-1c9a-4047-97c6-0bf59b2e0ae2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files in HR directory: 58000\n",
            "Total files in LR directory: 58000\n",
            "Training HR files: 0\n",
            "Training LR files: 0\n",
            "Test HR files: 0\n",
            "Test LR files: 0\n",
            "Missing in LR (train): 0 files\n",
            "Missing in HR (train): 0 files\n",
            "Missing in LR (test): 0 files\n",
            "Missing in HR (test): 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split files based on indices\n",
        "train_hr_files = sorted(hr_files)[:48000]\n",
        "train_lr_files = sorted(lr_files)[:48000]\n",
        "test_hr_files = sorted(hr_files)[48000:]\n",
        "test_lr_files = sorted(lr_files)[48000:]\n",
        "\n",
        "# Debugging counts\n",
        "print(f\"Training HR files: {len(train_hr_files)}\")\n",
        "print(f\"Training LR files: {len(train_lr_files)}\")\n",
        "print(f\"Test HR files: {len(test_hr_files)}\")\n",
        "print(f\"Test LR files: {len(test_lr_files)}\")\n",
        "\n",
        "# Verify counts\n",
        "assert len(train_hr_files) == len(train_lr_files) == 48000, \"Training pairs mismatch!\"\n",
        "assert len(test_hr_files) == len(test_lr_files) == 10000, \"Test pairs mismatch!\"\n",
        "\n",
        "# Verify filenames match\n",
        "assert all(hr.replace(\"_hr\", \"_lr\") == lr for hr, lr in zip(train_hr_files, train_lr_files)), \"Training filenames do not match!\"\n",
        "assert all(hr.replace(\"_hr\", \"_lr\") == lr for hr, lr in zip(test_hr_files, test_lr_files)), \"Test filenames do not match!\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CfryRAQVnK_",
        "outputId": "6a3cfced-7f03-4523-da15-95cbf5da92e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training HR files: 48000\n",
            "Training LR files: 48000\n",
            "Test HR files: 10000\n",
            "Test LR files: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hr_dir = \"/content/unified_dataset_extracted/super_resolution_hr\"\n",
        "lr_dir = \"/content/unified_dataset_extracted/super_resolution_lr\"\n",
        "\n",
        "# Load filenames\n",
        "hr_files = sorted(os.listdir(hr_dir))\n",
        "lr_files = sorted(os.listdir(lr_dir))\n",
        "\n",
        "# Verify counts\n",
        "assert len(hr_files) == len(lr_files) == 58000, \"Total pairs mismatch!\"\n",
        "\n",
        "# Split into training and testing\n",
        "train_hr_files = hr_files[:48000]\n",
        "train_lr_files = lr_files[:48000]\n",
        "test_hr_files = hr_files[48000:]\n",
        "test_lr_files = lr_files[48000:]\n",
        "\n",
        "# Verify training counts\n",
        "assert len(train_hr_files) == len(train_lr_files) == 48000, \"Training pairs mismatch!\"\n",
        "# Verify testing counts\n",
        "assert len(test_hr_files) == len(test_lr_files) == 10000, \"Test pairs mismatch!\"\n",
        "\n",
        "# Verify filenames match\n",
        "train_mismatched = [\n",
        "    (hr, lr) for hr, lr in zip(train_hr_files, train_lr_files)\n",
        "    if hr.replace(\"_hr\", \"_lr\") != lr\n",
        "]\n",
        "test_mismatched = [\n",
        "    (hr, lr) for hr, lr in zip(test_hr_files, test_lr_files)\n",
        "    if hr.replace(\"_hr\", \"_lr\") != lr\n",
        "]\n",
        "\n",
        "if not train_mismatched and not test_mismatched:\n",
        "    print(\"All pairs match!\")\n",
        "else:\n",
        "    print(f\"Training mismatches: {len(train_mismatched)}\")\n",
        "    print(f\"Test mismatches: {len(test_mismatched)}\")\n",
        "    if train_mismatched:\n",
        "        print(\"Sample training mismatches:\", train_mismatched[:5])\n",
        "    if test_mismatched:\n",
        "        print(\"Sample test mismatches:\", test_mismatched[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU7-5YWZWoEt",
        "outputId": "ffaeb62c-51d9-4898-f9cf-d39fb3846416"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All pairs match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to CIFAR-10 and CIFAR-100 in the unified dataset\n",
        "unified_base_dir = \"/content/unified_dataset_extracted\"\n",
        "cifar10_dir = os.path.join(unified_base_dir, \"data/cifar/cifar10\")\n",
        "cifar10_lr_dir = os.path.join(unified_base_dir, \"data/cifar/cifar10_lr\")\n",
        "cifar10_hr_dir = os.path.join(unified_base_dir, \"data/cifar/cifar10_hr\")\n",
        "cifar100_lr_dir = os.path.join(unified_base_dir, \"data/cifar/cifar100_lr\")\n",
        "cifar100_hr_dir = os.path.join(unified_base_dir, \"data/cifar/cifar100_hr\")\n",
        "\n",
        "# List of directories to remove\n",
        "dirs_to_remove = [cifar10_dir, cifar10_lr_dir, cifar10_hr_dir, cifar100_lr_dir, cifar100_hr_dir]\n",
        "\n",
        "# Remove directories if they exist\n",
        "for dir_path in dirs_to_remove:\n",
        "    if os.path.exists(dir_path):\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Deleted: {dir_path}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {dir_path}\")\n",
        "\n",
        "print(\"Old CIFAR datasets removed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL_qvnCzXcxu",
        "outputId": "230b64fe-fe8f-48b1-81b2-2ae5f7ce4abe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory not found: /content/unified_dataset_extracted/data/cifar/cifar10\n",
            "Deleted: /content/unified_dataset_extracted/data/cifar/cifar10_lr\n",
            "Deleted: /content/unified_dataset_extracted/data/cifar/cifar10_hr\n",
            "Deleted: /content/unified_dataset_extracted/data/cifar/cifar100_lr\n",
            "Deleted: /content/unified_dataset_extracted/data/cifar/cifar100_hr\n",
            "Old CIFAR datasets removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/unified_dataset_extracted\"\n",
        "\n",
        "# Function to collect statistics\n",
        "def get_dataset_statistics(base_dir):\n",
        "    dataset_stats = {}\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        if files:\n",
        "            dataset_stats[root] = len(files)\n",
        "    return dataset_stats\n",
        "\n",
        "# Collect statistics\n",
        "dataset_stats = get_dataset_statistics(base_dir)\n",
        "\n",
        "# Display summary\n",
        "total_files = sum(dataset_stats.values())\n",
        "print(f\"Total files in unified dataset: {total_files}\")\n",
        "print(\"\\nSample dataset statistics (first 10):\")\n",
        "for path, file_count in list(dataset_stats.items())[:10]:\n",
        "    print(f\"{path}: {file_count} files\")\n",
        "\n",
        "# Check for CIFAR-related directories\n",
        "print(\"\\nChecking for CIFAR-related directories...\")\n",
        "cifar_keywords = [\"cifar\", \"cifar10\", \"cifar100\"]\n",
        "cifar_dirs = [path for path in dataset_stats if any(keyword in path.lower() for keyword in cifar_keywords)]\n",
        "\n",
        "if cifar_dirs:\n",
        "    print(\"CIFAR-related directories found:\")\n",
        "    for path in cifar_dirs:\n",
        "        print(f\"- {path}\")\n",
        "else:\n",
        "    print(\"No CIFAR-related directories found in the dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYTiJ18NX1QM",
        "outputId": "b3bfd9f1-fcd7-40b4-ea96-b16249f7978c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files in unified dataset: 544492\n",
            "\n",
            "Sample dataset statistics (first 10):\n",
            "/content/unified_dataset_extracted/super_resolution_hr: 58000 files\n",
            "/content/unified_dataset_extracted/super_resolution_lr: 58000 files\n",
            "/content/unified_dataset_extracted/content/data/coco/train2017: 118287 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200: 2 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/test/images: 10000 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/val: 1 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/tiny-imagenet-200: 2 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/train/n01768244: 1 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/train/n01768244/images: 500 files\n",
            "/content/unified_dataset_extracted/content/data/tiny_imagenet/tiny-imagenet-200/train/n02002724: 1 files\n",
            "\n",
            "Checking for CIFAR-related directories...\n",
            "No CIFAR-related directories found in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Integrate New CIFAR-100 Data"
      ],
      "metadata": {
        "id": "IM2ihoUEYL9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_cifar_hr_dir = \"/content/unified_dataset_extracted/super_resolution_hr\"\n",
        "new_cifar_lr_dir = \"/content/unified_dataset_extracted/super_resolution_lr\"\n",
        "\n",
        "# Final target paths for integration with the unified dataset\n",
        "final_unified_hr_dir = \"/content/unified_dataset_extracted/content/super_resolution_hr\"\n",
        "final_unified_lr_dir = \"/content/unified_dataset_extracted/content/super_resolution_lr\"\n",
        "\n",
        "# Ensure target directories exist\n",
        "os.makedirs(final_unified_hr_dir, exist_ok=True)\n",
        "os.makedirs(final_unified_lr_dir, exist_ok=True)\n",
        "\n",
        "# Function that moves files to final dataset\n",
        "def integrate_cifar_data(source_dir, target_dir):\n",
        "    for filename in os.listdir(source_dir):\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "        target_path = os.path.join(target_dir, filename)\n",
        "        shutil.move(source_path, target_path)\n",
        "    print(f\"Integrated {len(os.listdir(target_dir))} files into {target_dir}\")\n",
        "\n",
        "# Integrate HR and LR datasets into their respective final locations\n",
        "integrate_cifar_data(new_cifar_hr_dir, final_unified_hr_dir)\n",
        "integrate_cifar_data(new_cifar_lr_dir, final_unified_lr_dir)\n",
        "\n",
        "print(\"\\nCIFAR-100 dataset successfully integrated into the unified dataset structure!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_7hJF8cYLSX",
        "outputId": "32772140-b05c-4d89-9566-639663668c10"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integrated 118000 files into /content/unified_dataset_extracted/content/super_resolution_hr\n",
            "Integrated 58000 files into /content/unified_dataset_extracted/content/super_resolution_lr\n",
            "\n",
            "CIFAR-100 dataset successfully integrated into the unified dataset structure!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VERIFY"
      ],
      "metadata": {
        "id": "Up4U2m7EZVGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List files that don't match the CIFAR-100 naming convention\n",
        "extra_hr_files = [f for f in hr_files if not f.endswith(\"_hr.png\")]\n",
        "\n",
        "print(f\"Number of extra files in HR directory: {len(extra_hr_files)}\")\n",
        "if extra_hr_files:\n",
        "    print(f\"Sample extra files: {extra_hr_files[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htmt2xXLZxXK",
        "outputId": "2342a839-3485-484b-caf1-f2ebddf9bb77"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of extra files in HR directory: 60000\n",
            "Sample extra files: ['cifar100_hr_0.png', 'cifar100_hr_1.png', 'cifar100_hr_10.png', 'cifar100_hr_100.png', 'cifar100_hr_1000.png', 'cifar100_hr_10000.png', 'cifar100_hr_10001.png', 'cifar100_hr_10002.png', 'cifar100_hr_10003.png', 'cifar100_hr_10004.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Remove extra CIFAR-100 files from HR directory\n",
        "extra_hr_files = [f for f in hr_files if f.startswith(\"cifar100_hr_\")]\n",
        "\n",
        "for file in extra_hr_files:\n",
        "    os.remove(os.path.join(hr_dir, file))\n",
        "\n",
        "print(f\"Removed {len(extra_hr_files)} extra files from HR directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1wcGSZYZ9Zl",
        "outputId": "ea9653e1-012c-4ead-9625-4a70d05f05e0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 60000 extra files from HR directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the directories after cleanup\n",
        "hr_files = sorted(os.listdir(hr_dir))\n",
        "lr_files = sorted(os.listdir(lr_dir))\n",
        "\n",
        "print(f\"Total files in HR directory after cleanup: {len(hr_files)}\")\n",
        "print(f\"Total files in LR directory after cleanup: {len(lr_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_k2hzLlaCHC",
        "outputId": "ccf5daf3-6130-41d4-c7be-47fd4f4c500c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files in HR directory after cleanup: 58000\n",
            "Total files in LR directory after cleanup: 58000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify matching pairs\n",
        "assert len(hr_files) == len(lr_files), \"Mismatch in number of HR and LR files!\"\n",
        "assert all(hr.replace(\"_hr\", \"_lr\") == lr for hr, lr in zip(hr_files, lr_files)), \"Filenames do not match between HR and LR!\"\n",
        "\n",
        "print(\"All HR and LR pairs match perfectly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQCQB4v-aFkn",
        "outputId": "a53c45db-b0f7-4aed-afc7-b6eb6b881816"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All HR and LR pairs match perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find DIV2K"
      ],
      "metadata": {
        "id": "A_lpmCHKeD8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/unified_dataset_extracted/content\"\n",
        "\n",
        "# Function to locate DIV2K data within the dataset\n",
        "def find_div2k_data(base_dir):\n",
        "    div2k_dirs = []\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        if \"div2k\" in root.lower():\n",
        "            div2k_dirs.append(root)\n",
        "\n",
        "    return div2k_dirs\n",
        "\n",
        "# Locate DIV2K directories\n",
        "div2k_dirs = find_div2k_data(base_dir)\n",
        "\n",
        "# Check results\n",
        "if div2k_dirs:\n",
        "    print(f\"Found DIV2K data in {len(div2k_dirs)} directories:\")\n",
        "    for dir_path in div2k_dirs:\n",
        "        print(f\"Directory: {dir_path}\")\n",
        "        sample_files = [f for f in os.listdir(dir_path) if f.endswith(\".img\")]\n",
        "        print(f\"Sample .img files ({len(sample_files)} found): {sample_files[:5]}\")\n",
        "else:\n",
        "    print(\"No DIV2K data found in the unified dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xpNagCYeDKb",
        "outputId": "7f9c2e2e-921f-4565-bd96-6a04e2394f92"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No DIV2K data found in the unified dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to locate any files or directories with \"DIV2K\" in their name\n",
        "def find_div2k_references(base_dir):\n",
        "    div2k_references = []\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for name in dirs + files:\n",
        "            if \"div2k\" in name.lower():\n",
        "                div2k_references.append(os.path.join(root, name))\n",
        "\n",
        "    return div2k_references\n",
        "\n",
        "# Search for any DIV2K-related references\n",
        "div2k_references = find_div2k_references(base_dir)\n",
        "\n",
        "# Display results\n",
        "if div2k_references:\n",
        "    print(f\"Found {len(div2k_references)} DIV2K-related files or directories:\")\n",
        "    for ref in div2k_references[:10]:  # Show first 10 references\n",
        "        print(ref)\n",
        "else:\n",
        "    print(\"No DIV2K-related files or directories found in the unified dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEkB6spYeM5B",
        "outputId": "a2cfd7f1-7bc9-497e-9776-74e35dc70999"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No DIV2K-related files or directories found in the unified dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/unified_dataset_extracted\"\n",
        "\n",
        "def summarize_data_sources(base_dir):\n",
        "    dataset_summary = {}\n",
        "\n",
        "    # Walk through the base directory\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        if files:\n",
        "            # Filter file extensions and count the number of files\n",
        "            img_files = [file for file in files if file.endswith('.img')]\n",
        "            txt_files = [file for file in files if file.endswith('.txt')]\n",
        "\n",
        "            # Store in summary\n",
        "            dataset_summary[root] = {\n",
        "                \"total_files\": len(files),\n",
        "                \"image_files\": len(img_files),\n",
        "                \"text_files\": len(txt_files),\n",
        "                \"other_files\": len(files) - len(img_files) - len(txt_files)\n",
        "            }\n",
        "\n",
        "    return dataset_summary\n",
        "\n",
        "# Run the summary function and display results\n",
        "dataset_summary = summarize_data_sources(base_dir)\n",
        "\n",
        "# Print summary of datasets\n",
        "print(f\"Data Sources in {base_dir}:\")\n",
        "for path, counts in dataset_summary.items():\n",
        "    print(f\"Directory: {path}\")\n",
        "    print(f\"  Total Files: {counts['total_files']}\")\n",
        "    print(f\"  Image Files (.img): {counts['image_files']}\")\n",
        "    print(f\"  Text Files (.txt): {counts['text_files']}\")\n",
        "    print(f\"  Other Files: {counts['other_files']}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "qBPvtSajfNtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to mystery directory\n",
        "mystery_dir = \"/content/unified_dataset_extracted/content/unified_dataset_extracted\"\n",
        "\n",
        "# List a sample of files\n",
        "mystery_files = sorted(os.listdir(mystery_dir))[:10]\n",
        "print(\"Sample Files in Mystery Directory:\")\n",
        "print(mystery_files)\n",
        "\n",
        "# Check properties of a few files\n",
        "print(\"\\nFile Properties:\")\n",
        "for file in mystery_files:\n",
        "    file_path = os.path.join(mystery_dir, file)\n",
        "    try:\n",
        "        # Load the .img file as a numpy array to check dimensions\n",
        "        img = np.fromfile(file_path, dtype=np.uint8)\n",
        "        print(f\"{file}: Shape={img.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvwj_KVdgD6j",
        "outputId": "c7393529-7c53-4e12-e55c-5cff7b642550"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Files in Mystery Directory:\n",
            "['coco_000000000009.img', 'coco_000000000030.img', 'coco_000000000036.img', 'coco_000000000042.img', 'coco_000000000049.img', 'coco_000000000064.img', 'coco_000000000071.img', 'coco_000000000073.img', 'coco_000000000074.img', 'coco_000000000077.img']\n",
            "\n",
            "File Properties:\n",
            "coco_000000000009.img: Shape=(224297,)\n",
            "coco_000000000030.img: Shape=(71463,)\n",
            "coco_000000000036.img: Shape=(260207,)\n",
            "coco_000000000042.img: Shape=(213308,)\n",
            "coco_000000000049.img: Shape=(124619,)\n",
            "coco_000000000064.img: Shape=(220869,)\n",
            "coco_000000000071.img: Shape=(214185,)\n",
            "coco_000000000073.img: Shape=(383651,)\n",
            "coco_000000000074.img: Shape=(176151,)\n",
            "coco_000000000077.img: Shape=(159213,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the last 10 files in the mystery directory\n",
        "mystery_files_last = sorted(os.listdir(mystery_dir))[-10:]\n",
        "print(\"Last 10 Files in Mystery Directory:\")\n",
        "print(mystery_files_last)\n",
        "\n",
        "# Check properties of the last 10 files\n",
        "print(\"\\nFile Properties for Last 10 Files:\")\n",
        "for file in mystery_files_last:\n",
        "    file_path = os.path.join(mystery_dir, file)\n",
        "    try:\n",
        "        # Load the .img file as a numpy array to check dimensions\n",
        "        img = np.fromfile(file_path, dtype=np.uint8)\n",
        "        print(f\"{file}: Shape={img.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpLbqt-ggaMi",
        "outputId": "ddb2cc2d-5d43-4713-f5b6-00fe12ab2792"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 10 Files in Mystery Directory:\n",
            "['tiny_imagenet_n12267677_90.img', 'tiny_imagenet_n12267677_91.img', 'tiny_imagenet_n12267677_92.img', 'tiny_imagenet_n12267677_93.img', 'tiny_imagenet_n12267677_94.img', 'tiny_imagenet_n12267677_95.img', 'tiny_imagenet_n12267677_96.img', 'tiny_imagenet_n12267677_97.img', 'tiny_imagenet_n12267677_98.img', 'tiny_imagenet_n12267677_99.img']\n",
            "\n",
            "File Properties for Last 10 Files:\n",
            "tiny_imagenet_n12267677_90.img: Shape=(1858,)\n",
            "tiny_imagenet_n12267677_91.img: Shape=(1564,)\n",
            "tiny_imagenet_n12267677_92.img: Shape=(1491,)\n",
            "tiny_imagenet_n12267677_93.img: Shape=(2675,)\n",
            "tiny_imagenet_n12267677_94.img: Shape=(1917,)\n",
            "tiny_imagenet_n12267677_95.img: Shape=(2630,)\n",
            "tiny_imagenet_n12267677_96.img: Shape=(2293,)\n",
            "tiny_imagenet_n12267677_97.img: Shape=(1877,)\n",
            "tiny_imagenet_n12267677_98.img: Shape=(1624,)\n",
            "tiny_imagenet_n12267677_99.img: Shape=(2090,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "root_hr_dir = \"/content/unified_dataset_extracted/super_resolution_hr\"\n",
        "root_lr_dir = \"/content/unified_dataset_extracted/super_resolution_lr\"\n",
        "nested_hr_dir = \"/content/unified_dataset_extracted/content/super_resolution_hr\"\n",
        "nested_lr_dir = \"/content/unified_dataset_extracted/content/super_resolution_lr\"\n",
        "\n",
        "# Function to list files in a directory\n",
        "def list_files(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        return []\n",
        "    return sorted([os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
        "\n",
        "# List files in each directory\n",
        "root_hr_files = list_files(root_hr_dir)\n",
        "root_lr_files = list_files(root_lr_dir)\n",
        "nested_hr_files = list_files(nested_hr_dir)\n",
        "nested_lr_files = list_files(nested_lr_dir)\n",
        "\n",
        "# Compare HR directories\n",
        "print(f\"Total HR files in root directory: {len(root_hr_files)}\")\n",
        "print(f\"Total HR files in nested directory: {len(nested_hr_files)}\")\n",
        "print(\"HR directories match!\" if root_hr_files == nested_hr_files else \"HR directories do not match!\")\n",
        "\n",
        "# Compare LR directories\n",
        "print(f\"Total LR files in root directory: {len(root_lr_files)}\")\n",
        "print(f\"Total LR files in nested directory: {len(nested_lr_files)}\")\n",
        "print(\"LR directories match!\" if root_lr_files == nested_lr_files else \"LR directories do not match!\")\n",
        "\n",
        "# Function to find mismatched files\n",
        "def find_mismatched_files(files1, files2):\n",
        "    set1 = set(os.path.basename(f) for f in files1)\n",
        "    set2 = set(os.path.basename(f) for f in files2)\n",
        "    return list(set1 - set2), list(set2 - set1)\n",
        "\n",
        "# Find mismatched HR files\n",
        "hr_missing_in_nested, hr_missing_in_root = find_mismatched_files(root_hr_files, nested_hr_files)\n",
        "print(f\"HR files missing in nested: {len(hr_missing_in_nested)}\")\n",
        "print(f\"HR files missing in root: {len(hr_missing_in_root)}\")\n",
        "\n",
        "# Find mismatched LR files\n",
        "lr_missing_in_nested, lr_missing_in_root = find_mismatched_files(root_lr_files, nested_lr_files)\n",
        "print(f\"LR files missing in nested: {len(lr_missing_in_nested)}\")\n",
        "print(f\"LR files missing in root: {len(lr_missing_in_root)}\")\n",
        "\n",
        "# Print sample mismatched files\n",
        "print(\"Sample HR files missing in nested:\", hr_missing_in_nested[:10])\n",
        "print(\"Sample HR files missing in root:\", hr_missing_in_root[:10])\n",
        "print(\"Sample LR files missing in nested:\", lr_missing_in_nested[:10])\n",
        "print(\"Sample LR files missing in root:\", lr_missing_in_root[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh4E2N94j95E",
        "outputId": "1ff04249-96af-4fc3-c7db-efa27d88f60c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total HR files in root directory: 0\n",
            "Total HR files in nested directory: 58000\n",
            "HR directories do not match!\n",
            "Total LR files in root directory: 0\n",
            "Total LR files in nested directory: 58000\n",
            "LR directories do not match!\n",
            "HR files missing in nested: 0\n",
            "HR files missing in root: 58000\n",
            "LR files missing in nested: 0\n",
            "LR files missing in root: 58000\n",
            "Sample HR files missing in nested: []\n",
            "Sample HR files missing in root: ['img_20089_hr.png', 'img_38419_hr.png', 'img_32981_hr.png', 'img_01878_hr.png', 'img_30333_hr.png', 'img_19770_hr.png', 'img_09495_hr.png', 'img_58484_hr.png', 'img_52961_hr.png', 'img_29142_hr.png']\n",
            "Sample LR files missing in nested: []\n",
            "Sample LR files missing in root: ['img_37131_lr.png', 'img_18486_lr.png', 'img_13046_lr.png', 'img_26742_lr.png', 'img_33234_lr.png', 'img_00251_lr.png', 'img_09394_lr.png', 'img_14866_lr.png', 'img_29245_lr.png', 'img_35958_lr.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_hr_dir = \"/content/unified_dataset_extracted/super_resolution_hr\"\n",
        "root_lr_dir = \"/content/unified_dataset_extracted/super_resolution_lr\"\n",
        "\n",
        "# Remove the empty directories\n",
        "if os.path.exists(root_hr_dir):\n",
        "    shutil.rmtree(root_hr_dir)\n",
        "    print(f\"Deleted empty directory: {root_hr_dir}\")\n",
        "\n",
        "if os.path.exists(root_lr_dir):\n",
        "    shutil.rmtree(root_lr_dir)\n",
        "    print(f\"Deleted empty directory: {root_lr_dir}\")\n",
        "\n",
        "print(\"Environment cleaned up successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdSXzc-UkWEI",
        "outputId": "b9e37b4d-c1ad-4c7c-9381-a7605fd9cca0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted empty directory: /content/unified_dataset_extracted/super_resolution_hr\n",
            "Deleted empty directory: /content/unified_dataset_extracted/super_resolution_lr\n",
            "Environment cleaned up successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dir = \"/content/unified_dataset_extracted/content/unified_dataset_extracted\"\n",
        "coco_dir = \"/content/unified_dataset_extracted/content/data/coco\"\n",
        "tiny_imagenet_dir = \"/content/unified_dataset_extracted/content/data/tiny_imagenet\"\n",
        "\n",
        "# Function to compare files between directories\n",
        "def compare_directories(dir1, dir2):\n",
        "    files1 = set(os.listdir(dir1)) if os.path.exists(dir1) else set()\n",
        "    files2 = set(os.listdir(dir2)) if os.path.exists(dir2) else set()\n",
        "    only_in_dir1 = files1 - files2\n",
        "    only_in_dir2 = files2 - files1\n",
        "    common_files = files1 & files2\n",
        "    return only_in_dir1, only_in_dir2, common_files\n",
        "\n",
        "# Compare combined with separate folders\n",
        "print(\"Comparing Combined Dataset with Separate Folders...\")\n",
        "\n",
        "# COCO\n",
        "coco_combined, coco_separate, coco_common = compare_directories(combined_dir, coco_dir)\n",
        "print(f\"COCO - Only in Combined: {len(coco_combined)}\")\n",
        "print(f\"COCO - Only in Separate: {len(coco_separate)}\")\n",
        "print(f\"COCO - Common: {len(coco_common)}\")\n",
        "\n",
        "# Tiny ImageNet\n",
        "tinynet_combined, tinynet_separate, tinynet_common = compare_directories(combined_dir, tiny_imagenet_dir)\n",
        "print(f\"Tiny ImageNet - Only in Combined: {len(tinynet_combined)}\")\n",
        "print(f\"Tiny ImageNet - Only in Separate: {len(tinynet_separate)}\")\n",
        "print(f\"Tiny ImageNet - Common: {len(tinynet_common)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6xvH-UUldqw",
        "outputId": "16d8825c-6ef3-484f-874e-5b43ad58e286"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Combined Dataset with Separate Folders...\n",
            "COCO - Only in Combined: 200001\n",
            "COCO - Only in Separate: 1\n",
            "COCO - Common: 0\n",
            "Tiny ImageNet - Only in Combined: 200001\n",
            "Tiny ImageNet - Only in Separate: 1\n",
            "Tiny ImageNet - Common: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "separate_coco_dir = \"/content/unified_dataset_extracted/content/data/coco\"\n",
        "separate_tinynet_dir = \"/content/unified_dataset_extracted/content/data/tiny_imagenet\"\n",
        "\n",
        "# Remove the incomplete separate folders\n",
        "if os.path.exists(separate_coco_dir):\n",
        "    shutil.rmtree(separate_coco_dir)\n",
        "    print(f\"Removed incomplete COCO folder: {separate_coco_dir}\")\n",
        "\n",
        "if os.path.exists(separate_tinynet_dir):\n",
        "    shutil.rmtree(separate_tinynet_dir)\n",
        "    print(f\"Removed incomplete Tiny ImageNet folder: {separate_tinynet_dir}\")\n",
        "\n",
        "# Check the remaining structure\n",
        "print(\"\\nFinal Dataset Structure:\")\n",
        "for root, dirs, files in os.walk(\"/content/unified_dataset_extracted\", topdown=True):\n",
        "    print(f\"Directory: {root}\")\n",
        "    print(f\"  Subdirectories: {dirs}\")\n",
        "    print(f\"  Files: {len(files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2YO-1Yilqtq",
        "outputId": "c9a154e4-2e9b-46f9-b2be-f3dfd2e17458"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed incomplete COCO folder: /content/unified_dataset_extracted/content/data/coco\n",
            "Removed incomplete Tiny ImageNet folder: /content/unified_dataset_extracted/content/data/tiny_imagenet\n",
            "\n",
            "Final Dataset Structure:\n",
            "Directory: /content/unified_dataset_extracted\n",
            "  Subdirectories: ['data', 'content']\n",
            "  Files: 0\n",
            "Directory: /content/unified_dataset_extracted/data\n",
            "  Subdirectories: ['cifar']\n",
            "  Files: 0\n",
            "Directory: /content/unified_dataset_extracted/data/cifar\n",
            "  Subdirectories: []\n",
            "  Files: 0\n",
            "Directory: /content/unified_dataset_extracted/content\n",
            "  Subdirectories: ['super_resolution_hr', 'data', 'super_resolution_lr', 'unified_dataset_extracted']\n",
            "  Files: 0\n",
            "Directory: /content/unified_dataset_extracted/content/super_resolution_hr\n",
            "  Subdirectories: []\n",
            "  Files: 58000\n",
            "Directory: /content/unified_dataset_extracted/content/data\n",
            "  Subdirectories: []\n",
            "  Files: 0\n",
            "Directory: /content/unified_dataset_extracted/content/super_resolution_lr\n",
            "  Subdirectories: []\n",
            "  Files: 58000\n",
            "Directory: /content/unified_dataset_extracted/content/unified_dataset_extracted\n",
            "  Subdirectories: ['content']\n",
            "  Files: 200000\n",
            "Directory: /content/unified_dataset_extracted/content/unified_dataset_extracted/content\n",
            "  Subdirectories: []\n",
            "  Files: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original path\n",
        "old_path = \"/content/unified_dataset_extracted/content/unified_dataset_extracted\"\n",
        "# New path\n",
        "new_path = \"/content/unified_dataset_extracted/content/tinynet_coco_combined\"\n",
        "\n",
        "# Rename the directory\n",
        "if os.path.exists(old_path):\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"Renamed directory:\\nFrom: {old_path}\\nTo: {new_path}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {old_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27oH9vcNmlUZ",
        "outputId": "4e20b92a-60bc-4915-f57c-3926aaef481e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed directory:\n",
            "From: /content/unified_dataset_extracted/content/unified_dataset_extracted\n",
            "To: /content/unified_dataset_extracted/content/tinynet_coco_combined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = \"/content/unified_dataset_extracted/data/cifar\"\n",
        "destination_path = \"/content/unified_dataset_extracted/content/cifar\"\n",
        "\n",
        "# Check if source exists\n",
        "if os.path.exists(source_path):\n",
        "    # Move the directory\n",
        "    shutil.move(source_path, destination_path)\n",
        "    print(f\"Moved 'cifar' directory:\\nFrom: {source_path}\\nTo: {destination_path}\")\n",
        "else:\n",
        "    print(f\"Source directory not found: {source_path}\")\n",
        "\n",
        "# Check if the source directory is now empty and remove it if needed\n",
        "parent_path = \"/content/unified_dataset_extracted/data\"\n",
        "if os.path.exists(parent_path) and not os.listdir(parent_path):\n",
        "    os.rmdir(parent_path)\n",
        "    print(f\"Removed empty parent directory: {parent_path}\")\n",
        "else:\n",
        "    print(f\"Parent directory still contains files or does not exist: {parent_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL_yUTb_mxDN",
        "outputId": "c9c6fd05-cf24-4d73-f44b-88c6a9e6c5f3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved 'cifar' directory:\n",
            "From: /content/unified_dataset_extracted/data/cifar\n",
            "To: /content/unified_dataset_extracted/content/cifar\n",
            "Removed empty parent directory: /content/unified_dataset_extracted/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "# Path to the unified dataset folder\n",
        "unified_dataset_path = \"/content/unified_dataset_extracted/content\"\n",
        "\n",
        "# Path for the new zip file\n",
        "zip_file_path = \"/content/drive/My Drive/perfectly_structured_dataset.zip\"\n",
        "\n",
        "# Function to create a zip file\n",
        "def create_zip_file(folder_path, zip_file_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "    print(f\"Zip file created successfully: {zip_file_path}\")\n",
        "\n",
        "# Create the zip file\n",
        "create_zip_file(unified_dataset_path, zip_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64neIemloTp3",
        "outputId": "6ffef052-1815-4363-ea21-6f63101fed18"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file created successfully: /content/drive/My Drive/perfectly_structured_dataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert CIFAR-100 .png Files to .img for Uniformity"
      ],
      "metadata": {
        "id": "nPSSKT4XvcPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "cifar_hr_dir = \"/content/unified_dataset_extracted/content/super_resolution_hr\"\n",
        "cifar_lr_dir = \"/content/unified_dataset_extracted/content/super_resolution_lr\"\n",
        "\n",
        "# Function to convert .png to .img\n",
        "def convert_png_to_img(input_dir):\n",
        "    for file in os.listdir(input_dir):\n",
        "        if file.endswith(\".png\"):\n",
        "            png_path = os.path.join(input_dir, file)\n",
        "            img_path = png_path.replace(\".png\", \".img\")\n",
        "            # Read PNG file\n",
        "            with open(png_path, \"rb\") as f:\n",
        "                data = f.read()\n",
        "            # Write to .img file\n",
        "            with open(img_path, \"wb\") as f:\n",
        "                f.write(data)\n",
        "            # Remove original .png file\n",
        "            os.remove(png_path)\n",
        "    print(f\"Converted all .png files to .img in {input_dir}\")\n",
        "\n",
        "# Convert HR and LR CIFAR data\n",
        "convert_png_to_img(cifar_hr_dir)\n",
        "convert_png_to_img(cifar_lr_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpdrLDkRvaiC",
        "outputId": "07300e7c-8e51-46eb-bd89-91d46f1a0762"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted all .png files to .img in /content/unified_dataset_extracted/content/super_resolution_hr\n",
            "Converted all .png files to .img in /content/unified_dataset_extracted/content/super_resolution_lr\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvOtXyDFu82On0tpGaNSpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}